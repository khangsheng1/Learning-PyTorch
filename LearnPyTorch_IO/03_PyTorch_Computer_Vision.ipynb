{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNCgEc4k7r3yFjoQZKUY3KN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khangsheng1/Learning-PyTorch/blob/main/LearnPyTorch_IO/03_PyTorch_Computer_Vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Computer Vision"
      ],
      "metadata": {
        "id": "W4M4ITT1hCuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "URL: https://www.learnpytorch.io/03_pytorch_computer_vision/"
      ],
      "metadata": {
        "id": "3mR5UtnVEWRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Example of Computer Vision](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-computer-vision-problems.png)"
      ],
      "metadata": {
        "id": "WSO-UhmuZVvM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[What's going to be covered](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-pytorch-computer-vision-workflow.png)\n",
        "\n",
        "| Topic| Contents|\n",
        "|---|---|\n",
        "| 0. Computer vision libraries in PyTorch| PyTorch has a bunch of built-in helpful computer vision libraries, let's check them out.|\n",
        "| 1. Load data| To practice computer vision, we'll start with some images of different pieces of clothing from [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist).|\n",
        "| 2. Prepare data| We've got some images, let's load them in with a [PyTorch DataLoader](https://pytorch.org/docs/stable/data.html) so we can use them with our training loop.     |\n",
        "| 3. Model 0: Building a baseline model| Here we'll create a multi-class classification model to learn patterns in the data, we'll also choose a **loss function**, **optimizer** and build a **training loop**. |\n",
        "| 4. Making predictions and evaluating model 0| Let's make some predictions with our baseline model and evaluate them.|\n",
        "| 5. Setup device agnostic code for future models     | It's best practice to write device-agnostic code, so let's set it up.|\n",
        "| 6. Model 1: Adding non-linearity| Experimenting is a large part of machine learning, let's try and improve upon our baseline model by adding non-linear layers. |\n",
        "| 7. Model 2: Convolutional Neural Network (CNN)      | Time to get computer vision specific and introduce the powerful convolutional neural network architecture.          |\n",
        "| 8. Comparing our models| We've built three different models, let's compare them. |\n",
        "| 9. Evaluating our best model   | Let's make some predictions on random images and evaluate our best model.                                          |\n",
        "| 10. Making a confusion matrix  | A confusion matrix is a great way to evaluate a classification model, let's see how we can make one. |\n",
        "| 11. Saving and loading the best performing model    | Since we might want to use our model for later, let's save it and make sure it loads back in correctly.            |\n"
      ],
      "metadata": {
        "id": "D6QpJfNKabGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 Computer vision libraries in PyTorch"
      ],
      "metadata": {
        "id": "zEYVHZhUg_xY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| PyTorch module                  | What does it do?                                                                                                             |\n",
        "|----------------------------------|------------------------------------------------------------------------------------------------------------------------------|\n",
        "| [torchvision](https://pytorch.org/vision/stable/index.html)                      | Contains datasets, model architectures and image transformations often used for computer vision problems.                   |\n",
        "| [torchvision.datasets](https://pytorch.org/vision/stable/datasets.html)              | Here you'll find many example computer vision datasets for a range of problems from image classification, object detection, image captioning, video classification and more. It also contains [a series of base classes for making custom datasets.](https://pytorch.org/vision/stable/datasets.html#base-classes-for-custom-datasets)  |\n",
        "| [torchvision.models](https://pytorch.org/vision/stable/models.html)                | This module contains well-performing and commonly used computer vision model architectures implemented in PyTorch, you can use these with your own problems.|\n",
        "| [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html)            | Often images need to be transformed (turned into numbers/processed/augmented) before being used with a model, common image transformations are found here.|\n",
        "| [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)          | Base dataset class for PyTorch.|\n",
        "| [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#module-torch.utils.data)       | Creates a Python iterable over a dataset (created with torch.utils.data.Dataset).|\n"
      ],
      "metadata": {
        "id": "F_zco9Jyh1fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyTorch\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Import torchvision\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Import matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check versions\n",
        "# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\n",
        "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dBUXx5RZRol",
        "outputId": "f2cafdf5-3e3c-4ac3-b9a1-10e591951e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.0+cu121\n",
            "torchvision version: 0.20.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Getting a dataset"
      ],
      "metadata": {
        "id": "IJk5Mjppj8CC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Original MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) contains examples of handwritten digits from 0 to 9"
      ],
      "metadata": {
        "id": "OHXD0462j_bP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we'll use the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset by Zalando Research\n",
        "\n",
        "\n",
        "*   Contains grayscale images of 10 different kinds of clothing\n",
        "\n",
        "[Image Example](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-fashion-mnist-slide.png)\n"
      ],
      "metadata": {
        "id": "5zK1WXy6kJA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup training data\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # where to download data to?\n",
        "    train=True, # get training data\n",
        "    download=True, # download data if it doesn't exist on disk\n",
        "    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
        "    target_transform=None # you can transform labels as well\n",
        ")\n",
        "\n",
        "# Setup testing data\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False, # get test data\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "3Wn40E7FkeWB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d483c619-54a9-4c6c-a5f6-d4a3926ad6b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:07<00:00, 3.43MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 304kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 262k/4.42M [00:00<00:12, 328kB/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See first training sample\n",
        "image, label = train_data[0]\n",
        "image, label"
      ],
      "metadata": {
        "id": "wRdGUlvoleM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Input and output shapes of a computer vision model"
      ],
      "metadata": {
        "id": "I8AxxCC4lpNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What's the shape of the image?\n",
        "image.shape"
      ],
      "metadata": {
        "id": "NsLRk_xKllB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1, 28, 28 = [color_channels=1, height=28, width=28]\n",
        "\n",
        "[Image](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-computer-vision-input-and-output-shapes.png)"
      ],
      "metadata": {
        "id": "coCXH6Y3lyVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many samples are there?\n",
        "len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)"
      ],
      "metadata": {
        "id": "_vzaGRL_lvXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See classes\n",
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "id": "fNzyXJeqnSHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Visualzing our data"
      ],
      "metadata": {
        "id": "gKQFUgldnWnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "image, label = train_data[0]\n",
        "print(f\"Image shape: {image.shape}\")\n",
        "plt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width)\n",
        "plt.title(label);"
      ],
      "metadata": {
        "id": "5YoSUbVCnVJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label]);"
      ],
      "metadata": {
        "id": "ZtO2dbJXnbIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot more images\n",
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "rows, cols = 4, 4\n",
        "for i in range(1, rows * cols + 1):\n",
        "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "    img, label = train_data[random_idx]\n",
        "    fig.add_subplot(rows, cols, i)\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    plt.title(class_names[label])\n",
        "    plt.axis(False);"
      ],
      "metadata": {
        "id": "gFQ2j4x1nixu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Prepare DataLoader"
      ],
      "metadata": {
        "id": "DqlQINkLo3M2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Batched data graphic](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-batching-fashionmnist.png)"
      ],
      "metadata": {
        "id": "zozZSqOVoaLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setup the batch size hyperparameter\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Turn datasets into iterables (batches)\n",
        "train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n",
        "                              batch_size=BATCH_SIZE, # how many samples per batch?\n",
        "                              shuffle=True #shuffle data every epoch?\n",
        "                              )\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False\n",
        "                             )\n",
        "\n",
        "# Let's check out what we've created\n",
        "print(f'DataLoader: {train_dataloader, test_dataloader}')\n",
        "print(f'Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}')\n",
        "print(f'Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}')"
      ],
      "metadata": {
        "id": "fnbHrHn8nqzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out what's inside the training dataloader\n",
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "train_features_batch.shape, train_labels_batch.shape"
      ],
      "metadata": {
        "id": "j13NpSLZp0Ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a sample\n",
        "torch.manual_seed(42)\n",
        "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
        "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
        "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "plt.axis(\"Off\");\n",
        "print(f\"Image size: {img.shape}\")\n",
        "print(f\"Label: {label}, label size: {label.shape}\")"
      ],
      "metadata": {
        "id": "9Lg103MwqQP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Model 0: Build a baseline model"
      ],
      "metadata": {
        "id": "ZdQ_JtHoqyXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flatten layer\n",
        "flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n",
        "\n",
        "# Get a single sample\n",
        "x = train_features_batch[0]\n",
        "\n",
        "# Flatten the sample\n",
        "output = flatten_model(x) # perform forward pass\n",
        "\n",
        "# Print out what happened\n",
        "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")\n",
        "\n",
        "# Try uncommenting below and see what happens\n",
        "#print(x)\n",
        "#print(output)"
      ],
      "metadata": {
        "id": "9_KSNwNqqY-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class FashionMNISTModelV0(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Flatten(), # neural networks like their inputs in vector form\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "NjQc_Vrhso4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Need to setup model with input parameters\n",
        "model_0 = FashionMNISTModelV0(input_shape=784, # one for every pixel (28x28)\n",
        "    hidden_units=10, # how many units in the hidden layer\n",
        "    output_shape=len(class_names) # one for every class\n",
        ")\n",
        "model_0.to(\"cpu\") # keep model on CPU to begin with"
      ],
      "metadata": {
        "id": "c7g-8ZaVuxzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Setup loss, optimizer and evluation metrics"
      ],
      "metadata": {
        "id": "yKRYpc81vDyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  # Note: you need the \"raw\" GitHub URL for this to work\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)"
      ],
      "metadata": {
        "id": "bODrvEdcu_Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import accuracy metric\n",
        "from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "Yo0kI6afwVPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Creating a function to time our experiment"
      ],
      "metadata": {
        "id": "t54G7W30wzqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "def print_train_time(start: float, end: float, device: torch.device = None):\n",
        "    \"\"\"Prints difference between start and end time.\n",
        "\n",
        "    Args:\n",
        "        start (float): Start time of computation (preferred in timeit format).\n",
        "        end (float): End time of computation.\n",
        "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        float: time between start and end in seconds (higher is longer).\n",
        "    \"\"\"\n",
        "    total_time = end - start\n",
        "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "    return total_time"
      ],
      "metadata": {
        "id": "flartr5RwwwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Creating a training loop and training a model on batches of data"
      ],
      "metadata": {
        "id": "1pTe0bRnw9M0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set the seed and start the timer\n",
        "torch.manual_seed(42)\n",
        "train_time_start_on_cpu = timer()\n",
        "\n",
        "# Set the number of epochs (we'll keep this small for faster training times)\n",
        "epochs = 3\n",
        "\n",
        "# Create training and testing loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n-------\")\n",
        "    ### Training\n",
        "    train_loss = 0\n",
        "    # Add a loop to loop through training batches\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "        model_0.train()\n",
        "        # 1. Forward pass\n",
        "        y_pred = model_0(X)\n",
        "\n",
        "        # 2. Calculate loss (per batch)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss # accumulatively add up the loss per epoch\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print out how many samples have been seen\n",
        "        if batch % 400 == 0:\n",
        "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
        "    train_loss /= len(train_dataloader)\n",
        "\n",
        "    ### Testing\n",
        "    # Setup variables for accumulatively adding up loss and accuracy\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model_0.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in test_dataloader:\n",
        "            # 1. Forward pass\n",
        "            test_pred = model_0(X)\n",
        "\n",
        "            # 2. Calculate loss (accumulatively)\n",
        "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
        "\n",
        "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
        "        # Divide total test loss by length of test dataloader (per batch)\n",
        "        test_loss /= len(test_dataloader)\n",
        "\n",
        "        # Divide total accuracy by length of test dataloader (per batch)\n",
        "        test_acc /= len(test_dataloader)\n",
        "\n",
        "    ## Print out what's happening\n",
        "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
        "\n",
        "# Calculate training time\n",
        "train_time_end_on_cpu = timer()\n",
        "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,\n",
        "                                           end=train_time_end_on_cpu,\n",
        "                                           device=str(next(model_0.parameters()).device))"
      ],
      "metadata": {
        "id": "tjQb0M5rw8Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Make predictions and get Model 0 results"
      ],
      "metadata": {
        "id": "I7RA5XWuzDQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn):\n",
        "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
        "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
        "        loss_fn (torch.nn.Module): The loss function of model.\n",
        "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
        "\n",
        "    Returns:\n",
        "        (dict): Results of model making predictions on data_loader.\n",
        "    \"\"\"\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Make predictions with the model\n",
        "            y_pred = model(X)\n",
        "\n",
        "            # Accumulate the loss and accuracy values per batch\n",
        "            loss += loss_fn(y_pred, y)\n",
        "            acc += accuracy_fn(y_true=y,\n",
        "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
        "\n",
        "        # Scale loss and acc to find the average loss/acc per batch\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "\n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}\n",
        "\n",
        "# Calculate model 0 results on test dataset\n",
        "model_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
        ")\n",
        "model_0_results"
      ],
      "metadata": {
        "id": "LNwrqra5xlW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 Setup device agnostic-code (for using a GPU if there is one)"
      ],
      "metadata": {
        "id": "Hh0iPcTa0wTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "OwagueLx0nHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 Model 1: Building a better model with non-linearity"
      ],
      "metadata": {
        "id": "VyQbaAWy05Ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model with non-linear and linear layers\n",
        "class FashionMNISTModelV1(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Flatten(), # flatten inputs into single vector\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "FALZvNYG03uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_1 = FashionMNISTModelV1(input_shape=784, # number of input features\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names) # number of output classes desired\n",
        ").to(device) # send model to GPU if it's available\n",
        "next(model_1.parameters()).device # check model device"
      ],
      "metadata": {
        "id": "zExISp2C1Oi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Setup loss, optimizer and evlauation metrics"
      ],
      "metadata": {
        "id": "ytUOhsVQ1TA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import accuracy_fn\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(),\n",
        "                            lr=0.1)\n",
        "\n",
        "accuracy_fn"
      ],
      "metadata": {
        "id": "-QtmLO_L1Qhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Functionizing training and test loops"
      ],
      "metadata": {
        "id": "51EbKRfI1Yq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device = device):\n",
        "    train_loss, train_acc = 0, 0\n",
        "    model.to(device)\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "        # Send data to GPU\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. Calculate loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss\n",
        "        train_acc += accuracy_fn(y_true=y,\n",
        "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate loss and accuracy per epoch and print out what's happening\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "def test_step(data_loader: torch.utils.data.DataLoader,\n",
        "              model: torch.nn.Module,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device = device):\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.to(device)\n",
        "    model.eval() # put model in eval mode\n",
        "    # Turn on inference context manager\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Send data to GPU\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            test_pred = model(X)\n",
        "\n",
        "            # 2. Calculate loss and accuracy\n",
        "            test_loss += loss_fn(test_pred, y)\n",
        "            test_acc += accuracy_fn(y_true=y,\n",
        "                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
        "            )\n",
        "\n",
        "        # Adjust metrics and print out\n",
        "        test_loss /= len(data_loader)\n",
        "        test_acc /= len(data_loader)\n",
        "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
      ],
      "metadata": {
        "id": "VnBaS-6D1Xs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Measure time\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_on_gpu = timer()\n",
        "\n",
        "epochs = 3\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n---------\")\n",
        "    train_step(data_loader=train_dataloader,\n",
        "        model=model_1,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        accuracy_fn=accuracy_fn\n",
        "    )\n",
        "    test_step(data_loader=test_dataloader,\n",
        "        model=model_1,\n",
        "        loss_fn=loss_fn,\n",
        "        accuracy_fn=accuracy_fn\n",
        "    )\n",
        "\n",
        "train_time_end_on_gpu = timer()\n",
        "total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n",
        "                                            end=train_time_end_on_gpu,\n",
        "                                            device=device)"
      ],
      "metadata": {
        "id": "Qw5vYU6N2pES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Note: This will error due to `eval_model()` not using device agnostic code\n",
        "model_1_results = eval_model(model=model_1,\n",
        "    data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn,\n",
        "    accuracy_fn=accuracy_fn)\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "X5EztueJ4XYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move values to device\n",
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn,\n",
        "               device: torch.device = device):\n",
        "    \"\"\"Evaluates a given model on a given dataset.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
        "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
        "        loss_fn (torch.nn.Module): The loss function of model.\n",
        "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
        "        device (str, optional): Target device to compute on. Defaults to device.\n",
        "\n",
        "    Returns:\n",
        "        (dict): Results of model making predictions on data_loader.\n",
        "    \"\"\"\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Send data to the target device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred = model(X)\n",
        "            loss += loss_fn(y_pred, y)\n",
        "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "        # Scale loss and acc\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}\n",
        "\n",
        "# Calculate model 1 results with device-agnostic code\n",
        "model_1_results = eval_model(model=model_1, data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn, accuracy_fn=accuracy_fn,\n",
        "    device=device\n",
        ")\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "eoqLVpcP5sSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check baseline results\n",
        "model_0_results"
      ],
      "metadata": {
        "id": "fxEdkNBk55bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 Model 2: Building a Convoluitonal Neural Network (CNN)"
      ],
      "metadata": {
        "id": "2JhbAK5H64DZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guide for which models to use:"
      ],
      "metadata": {
        "id": "lB_bWkJCG-ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|**Problem type**|**Model to use (generally)**|**Code example**|\n",
        "|--|--|--|\n",
        "|Structured data (Excel spreadsheets, row and column data)|Gradient boosted models, Random Forests, XGBoost|[sklearn.ensemble](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble), [XGBoost library](https://xgboost.readthedocs.io/en/stable/)|\n",
        "|Unstructured data (images, audio, language)|Convolutional Neural Networks, Transformers|[torchvision.models](https://pytorch.org/vision/stable/models.html), [HuggingFace Transformers](https://huggingface.co/docs/transformers/index)|"
      ],
      "metadata": {
        "id": "wynElC4h7QDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[CNN Explainer Website to help visualize what CNN's do](https://poloclub.github.io/cnn-explainer/)"
      ],
      "metadata": {
        "id": "o3YOD5M_HNjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a convolutional neural network\n",
        "class FashionMNISTModelV2(nn.Module):\n",
        "    \"\"\"\n",
        "    Model architecture copying TinyVGG from:\n",
        "    https://poloclub.github.io/cnn-explainer/\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3, # how big is the square that's going over the image?\n",
        "                      stride=1, # default\n",
        "                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2) # default stride value is same as kernel_size\n",
        "        )\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            # Where did this in_features shape come from?\n",
        "            # It's because each layer of our network compresses and changes the shape of our input data.\n",
        "            nn.Linear(in_features=hidden_units*7*7,\n",
        "                      out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.block_1(x)\n",
        "        # print(x.shape)\n",
        "        x = self.block_2(x)\n",
        "        # print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        # print(x.shape)\n",
        "        return x\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model_2 = FashionMNISTModelV2(input_shape=1,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names)).to(device)\n",
        "model_2"
      ],
      "metadata": {
        "id": "r20H86Us6XXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1 Stepping through `nn.Conv2d()`"
      ],
      "metadata": {
        "id": "DVjiLXNiJO84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create sample batch of random numbers with same size as image batch\n",
        "images = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\n",
        "test_image = images[0] # get a single image for testing\n",
        "print(f\"Image batch shape: {images.shape} -> [batch_size, color_channels, height, width]\")\n",
        "print(f\"Single image shape: {test_image.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Single image pixel values:\\n{test_image}\")"
      ],
      "metadata": {
        "id": "Up8WisNrJGRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Understanding Hyperparameters of a Conv2d layer](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-conv2d-layer.gif)"
      ],
      "metadata": {
        "id": "Hv5PKeaOKrTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a convolutional layer with same dimensions as TinyVGG\n",
        "# (try changing any of the parameters and see what happens)\n",
        "conv_layer = nn.Conv2d(in_channels=3,\n",
        "                       out_channels=10,\n",
        "                       kernel_size=3,\n",
        "                       stride=1,\n",
        "                       padding=0) # also try using \"valid\" or \"same\" here\n",
        "\n",
        "# Pass the data through the convolutional layer\n",
        "conv_layer(test_image) # Note: If running PyTorch <1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input)"
      ],
      "metadata": {
        "id": "40wLbwZ2KPiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add extra dimension to test image\n",
        "test_image.unsqueeze(dim=0).shape"
      ],
      "metadata": {
        "id": "bTWR6Y0hK_I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass test image with extra dimension through conv_layer\n",
        "conv_layer(test_image.unsqueeze(dim=0)).shape"
      ],
      "metadata": {
        "id": "42JyntVyLJjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# Create a new conv_layer with different values (try setting these to whatever you like)\n",
        "conv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image\n",
        "                         out_channels=10,\n",
        "                         kernel_size=(5, 5), # kernel is usually a square so a tuple also works\n",
        "                         stride=2,\n",
        "                         padding=0)\n",
        "\n",
        "# Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input)\n",
        "conv_layer_2(test_image.unsqueeze(dim=0)).shape"
      ],
      "metadata": {
        "id": "fiMeNwzBLMqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the conv_layer_2 internal parameters\n",
        "print(conv_layer_2.state_dict())"
      ],
      "metadata": {
        "id": "5RX3nn31IFUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get shapes of weight and bias tensors within conv_layer_2\n",
        "print(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -> [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\")\n",
        "print(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -> [out_channels=10]\")"
      ],
      "metadata": {
        "id": "lXj-bw8jIHdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2 Stepping through `nn.MaxPool2d()`"
      ],
      "metadata": {
        "id": "KJ9zCdk8IZuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out original image shape without and with unsqueezed dimension\n",
        "print(f\"Test image original shape: {test_image.shape}\")\n",
        "print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")\n",
        "\n",
        "# Create a sample nn.MaxPoo2d() layer\n",
        "max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "# Pass data through just the conv_layer\n",
        "test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\n",
        "print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n",
        "\n",
        "# Pass data through the max pool layer\n",
        "test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\n",
        "print(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")"
      ],
      "metadata": {
        "id": "2iXZpC50IKYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# Create a random tensor with a similar number of dimensions to our images\n",
        "random_tensor = torch.randn(size=(1, 1, 2, 2))\n",
        "print(f\"Random tensor:\\n{random_tensor}\")\n",
        "print(f\"Random tensor shape: {random_tensor.shape}\")\n",
        "\n",
        "# Create a max pool layer\n",
        "max_pool_layer = nn.MaxPool2d(kernel_size=2) # see what happens when you change the kernel_size value\n",
        "\n",
        "# Pass the random tensor through the max pool layer\n",
        "max_pool_tensor = max_pool_layer(random_tensor)\n",
        "print(f\"\\nMax pool tensor:\\n{max_pool_tensor} <- this is the maximum value from random_tensor\")\n",
        "print(f\"Max pool tensor shape: {max_pool_tensor.shape}\")"
      ],
      "metadata": {
        "id": "hvI7aKxUIfjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Visual represenation of what neural networks are doing](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-conv-net-as-compression.png)"
      ],
      "metadata": {
        "id": "P-1E5szpJiqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise: What do you think the [nn.AvgPool2d()](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html) layer does? Try making a random tensor like we did above and passing it through. Check the input and output shapes as well as the input and output values."
      ],
      "metadata": {
        "id": "PbEMhnmLJ9vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random tensor with a similar number of dimensions to our images\n",
        "random_tensor = torch.randn(size=(1, 1, 10, 10))\n",
        "print(f\"Random tensor:\\n{random_tensor}\")\n",
        "print(f\"Random tensor shape: {random_tensor.shape}\")\n",
        "\n",
        "# Create a avg pool layer\n",
        "avg_pool_layer = nn.AvgPool2d(kernel_size=2) # see what happens when you change the kernel_size value\n",
        "# Pass the random tensor through the avg pool layer\n",
        "avg_pool_tensor = avg_pool_layer(random_tensor)\n",
        "print(f\"\\navg pool tensor:\\n{avg_pool_tensor} <- this is the average value from random_tensor\")\n",
        "print(f\"avg pool tensor shape: {avg_pool_tensor.shape}\")"
      ],
      "metadata": {
        "id": "Vi94Rf51JU7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3 Setup a loss function and optimizer for `model_2`"
      ],
      "metadata": {
        "id": "7cEB0PrwKhBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_2.parameters(),\n",
        "                             lr=0.1)"
      ],
      "metadata": {
        "id": "ZW1hmQMcKG2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4 Training and testing `model_2` using out training and test functions"
      ],
      "metadata": {
        "id": "BGC-WDeuK0Sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Measure time\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_model_2 = timer()\n",
        "\n",
        "# Train and test model\n",
        "epochs = 3\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n---------\")\n",
        "    train_step(data_loader=train_dataloader,\n",
        "        model=model_2,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        accuracy_fn=accuracy_fn,\n",
        "        device=device\n",
        "    )\n",
        "    test_step(data_loader=test_dataloader,\n",
        "        model=model_2,\n",
        "        loss_fn=loss_fn,\n",
        "        accuracy_fn=accuracy_fn,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "train_time_end_model_2 = timer()\n",
        "total_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n",
        "                                           end=train_time_end_model_2,\n",
        "                                           device=device)"
      ],
      "metadata": {
        "id": "OOjfvOPbKzSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model_2 results\n",
        "model_2_results = eval_model(\n",
        "    model=model_2,\n",
        "    data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn,\n",
        "    accuracy_fn=accuracy_fn\n",
        ")\n",
        "model_2_results"
      ],
      "metadata": {
        "id": "WT2ouFkcLByj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 Compare model results and triaing time"
      ],
      "metadata": {
        "id": "OWdEgfwALXci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "compare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results])\n",
        "compare_results"
      ],
      "metadata": {
        "id": "1uIpZSAFLVcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add training times to results comparison\n",
        "compare_results[\"training_time\"] = [total_train_time_model_0,\n",
        "                                    total_train_time_model_1,\n",
        "                                    total_train_time_model_2]\n",
        "compare_results"
      ],
      "metadata": {
        "id": "aNnlOQ0ILqxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize our model results\n",
        "compare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\")\n",
        "plt.xlabel(\"accuracy (%)\")\n",
        "plt.ylabel(\"model\");"
      ],
      "metadata": {
        "id": "usK819YfLwBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9 Make and evlauate random predictions with best model"
      ],
      "metadata": {
        "id": "S176DiXpMGjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n",
        "    pred_probs = []\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for sample in data:\n",
        "            # Prepare sample\n",
        "            sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device\n",
        "\n",
        "            # Forward pass (model outputs raw logit)\n",
        "            pred_logit = model(sample)\n",
        "\n",
        "            # Get prediction probability (logit -> prediction probability)\n",
        "            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 1, so can perform on dim=0)\n",
        "\n",
        "            # Get pred_prob off GPU for further calculations\n",
        "            pred_probs.append(pred_prob.cpu())\n",
        "\n",
        "    # Stack the pred_probs to turn list into a tensor\n",
        "    return torch.stack(pred_probs)"
      ],
      "metadata": {
        "id": "bW2iAk_WMEGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "test_samples = []\n",
        "test_labels = []\n",
        "for sample, label in random.sample(list(test_data), k=9):\n",
        "    test_samples.append(sample)\n",
        "    test_labels.append(label)\n",
        "\n",
        "# View the first test sample shape and label\n",
        "print(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\")"
      ],
      "metadata": {
        "id": "5jMBzOdpMXcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test samples with model 2\n",
        "pred_probs= make_predictions(model=model_2,\n",
        "                             data=test_samples)\n",
        "\n",
        "# View first two prediction probabilities list\n",
        "pred_probs[:2]"
      ],
      "metadata": {
        "id": "d-lSCMmPMcYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test samples with model 2\n",
        "pred_probs= make_predictions(model=model_2,\n",
        "                             data=test_samples)\n",
        "\n",
        "# View first two prediction probabilities list\n",
        "pred_probs[:2]"
      ],
      "metadata": {
        "id": "HSwNmtm_MggV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the prediction probabilities into prediction labels by taking the argmax()\n",
        "pred_classes = pred_probs.argmax(dim=1)\n",
        "pred_classes"
      ],
      "metadata": {
        "id": "1gVbU28VMkfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Are our predictions in the same form as our test labels?\n",
        "test_labels, pred_classes"
      ],
      "metadata": {
        "id": "OqWib2fmMok7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot predictions\n",
        "plt.figure(figsize=(9, 9))\n",
        "nrows = 3\n",
        "ncols = 3\n",
        "for i, sample in enumerate(test_samples):\n",
        "  # Create a subplot\n",
        "  plt.subplot(nrows, ncols, i+1)\n",
        "\n",
        "  # Plot the target image\n",
        "  plt.imshow(sample.squeeze(), cmap=\"gray\")\n",
        "\n",
        "  # Find the prediction label (in text form, e.g. \"Sandal\")\n",
        "  pred_label = class_names[pred_classes[i]]\n",
        "\n",
        "  # Get the truth label (in text form, e.g. \"T-shirt\")\n",
        "  truth_label = class_names[test_labels[i]]\n",
        "\n",
        "  # Create the title text of the plot\n",
        "  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n",
        "\n",
        "  # Check for equality and change title colour accordingly\n",
        "  if pred_label == truth_label:\n",
        "      plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n",
        "  else:\n",
        "      plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n",
        "  plt.axis(False);"
      ],
      "metadata": {
        "id": "Kz-2xCjKMrbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 Making a confusion matrix for further prediction evaluation"
      ],
      "metadata": {
        "id": "OXN5Z0raNCow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. Make predictions with trained model\n",
        "y_preds = []\n",
        "model_2.eval()\n",
        "with torch.inference_mode():\n",
        "  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n",
        "    # Send data and targets to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    # Do the forward pass\n",
        "    y_logit = model_2(X)\n",
        "    # Turn predictions from logits -> prediction probabilities -> predictions labels\n",
        "    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 32, so can perform on dim=1)\n",
        "    # Put predictions on CPU for evaluation\n",
        "    y_preds.append(y_pred.cpu())\n",
        "# Concatenate list of predictions into a tensor\n",
        "y_pred_tensor = torch.cat(y_preds)"
      ],
      "metadata": {
        "id": "3x5NGT-SM3ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See if torchmetrics exists, if not, install it\n",
        "try:\n",
        "    import torchmetrics, mlxtend\n",
        "    print(f\"mlxtend version: {mlxtend.__version__}\")\n",
        "    assert int(mlxtend.__version__.split(\".\")[1]) >= 19, \"mlxtend verison should be 0.19.0 or higher\"\n",
        "except:\n",
        "    !pip install -q torchmetrics -U mlxtend # <- Note: If you're using Google Colab, this may require restarting the runtime\n",
        "    import torchmetrics, mlxtend\n",
        "    print(f\"mlxtend version: {mlxtend.__version__}\")"
      ],
      "metadata": {
        "id": "kk403EbxNUv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import mlxtend upgraded version\n",
        "import mlxtend\n",
        "print(mlxtend.__version__)\n",
        "assert int(mlxtend.__version__.split(\".\")[1]) >= 19 # should be version 0.19.0 or higher"
      ],
      "metadata": {
        "id": "2I4dg7L9Nhgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import ConfusionMatrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "# 2. Setup confusion matrix instance and compare predictions to targets\n",
        "confmat = ConfusionMatrix(num_classes=len(class_names), task='multiclass')\n",
        "confmat_tensor = confmat(preds=y_pred_tensor,\n",
        "                         target=test_data.targets)\n",
        "\n",
        "# 3. Plot the confusion matrix\n",
        "fig, ax = plot_confusion_matrix(\n",
        "    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy\n",
        "    class_names=class_names, # turn the row and column labels into class names\n",
        "    figsize=(10, 7)\n",
        ");"
      ],
      "metadata": {
        "id": "aSG1NqBTNmN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise: Use the trained `model_2` to make predictions on the test FashionMNIST dataset. Then plot some predictions where the model was wrong alongside what the label of the image should've been. After visualizing these predictions do you think it's more of a modelling error or a data error? As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?"
      ],
      "metadata": {
        "id": "8eTYQtq-OEwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "test_samples = []\n",
        "test_labels = []\n",
        "for sample, label in random.sample(list(test_data), k=9000):\n",
        "    test_samples.append(sample)\n",
        "    test_labels.append(label)\n",
        "\n",
        "# View the first test sample shape and label\n",
        "print(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\")"
      ],
      "metadata": {
        "id": "40pkCQqfQ7oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test samples with model 2\n",
        "pred_probs = []\n",
        "pred_probs= make_predictions(model=model_2,\n",
        "                             data=test_samples)\n",
        "\n",
        "# View first two prediction probabilities list\n",
        "pred_probs[:2]"
      ],
      "metadata": {
        "id": "h9dFrNEuQuoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the prediction probabilities into prediction labels by taking the argmax()\n",
        "pred_classes = pred_probs.argmax(dim=1)\n",
        "pred_classes"
      ],
      "metadata": {
        "id": "NmnHbwwERHVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Are our predictions in the same form as our test labels?\n",
        "# test_labels, pred_classes"
      ],
      "metadata": {
        "id": "H3g_M-WRRJyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Identify incorrect predictions\n",
        "incorrect_indices = [i for i in range(len(test_samples)) if class_names[pred_classes[i]] != class_names[test_labels[i]]]\n",
        "\n",
        "# Check if there are at least 9 incorrect predictions\n",
        "num_samples_to_plot = min(9, len(incorrect_indices))\n",
        "\n",
        "if num_samples_to_plot > 0:\n",
        "    # Randomly select 9 (or fewer if there are less than 9) incorrect predictions\n",
        "    selected_indices = random.sample(incorrect_indices, num_samples_to_plot)\n",
        "\n",
        "    # Plot selected incorrect predictions\n",
        "    plt.figure(figsize=(9, 9))\n",
        "    nrows = 3\n",
        "    ncols = 3\n",
        "\n",
        "    for plot_idx, sample_idx in enumerate(selected_indices):\n",
        "        # Get the test sample and labels\n",
        "        sample = test_samples[sample_idx]\n",
        "        pred_label = class_names[pred_classes[sample_idx]]\n",
        "        truth_label = class_names[test_labels[sample_idx]]\n",
        "\n",
        "        # Create a subplot\n",
        "        plt.subplot(nrows, ncols, plot_idx + 1)\n",
        "\n",
        "        # Plot the image\n",
        "        plt.imshow(sample.squeeze(), cmap=\"gray\")\n",
        "\n",
        "        # Set the title with red color for incorrect prediction\n",
        "        title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n",
        "        plt.title(title_text, fontsize=10, c=\"r\")\n",
        "        plt.axis(False)\n",
        "\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No incorrect predictions found.\")\n"
      ],
      "metadata": {
        "id": "17nYZNNvNpJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11 Savle and load best performing model"
      ],
      "metadata": {
        "id": "6TZi7W6_Rilu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, # create parent directories if needed\n",
        "                 exist_ok=True # if models directory already exists, don't error\n",
        ")\n",
        "\n",
        "# Create model save path\n",
        "MODEL_NAME = \"03_pytorch_computer_vision_model_2.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "# Save the model state dict\n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_2.state_dict(), # only saving the state_dict() only saves the learned parameters\n",
        "           f=MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "id": "IXakzPv0QD2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new instance of FashionMNISTModelV2 (the same class as our saved state_dict())\n",
        "# Note: loading model will error if the shapes here aren't the same as the saved version\n",
        "loaded_model_2 = FashionMNISTModelV2(input_shape=1,\n",
        "                                    hidden_units=10, # try changing this to 128 and seeing what happens\n",
        "                                    output_shape=10)\n",
        "\n",
        "# Load in the saved state_dict()\n",
        "loaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
        "\n",
        "# Send model to GPU\n",
        "loaded_model_2 = loaded_model_2.to(device)"
      ],
      "metadata": {
        "id": "iSR2fXBDRtYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate loaded model\n",
        "torch.manual_seed(42)\n",
        "\n",
        "loaded_model_2_results = eval_model(\n",
        "    model=loaded_model_2,\n",
        "    data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn,\n",
        "    accuracy_fn=accuracy_fn\n",
        ")\n",
        "\n",
        "loaded_model_2_results"
      ],
      "metadata": {
        "id": "hgdYVDz8RpUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_results"
      ],
      "metadata": {
        "id": "aHxGZQMjRvcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check to see if results are close to each other (if they are very far away, there may be an error)\n",
        "torch.isclose(torch.tensor(model_2_results[\"model_loss\"]),\n",
        "              torch.tensor(loaded_model_2_results[\"model_loss\"]),\n",
        "              atol=1e-08, # absolute tolerance\n",
        "              rtol=0.0001) # relative tolerance"
      ],
      "metadata": {
        "id": "th1Hk7AiRzNl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}